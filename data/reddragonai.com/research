<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="img/2019-08-01_EMNLP-2019-workshop_TextGraphs_v07_with-logo.png"/><link rel="preload" as="image" href="img/2019-08-01_EMNLP-2019-workshop_WNGT_Paraphrasing_with_Large_LMs_cover_final2.png"/><link rel="preload" as="image" href="img/2018-10-26_NIPS-2018-workshop_MLPCD2-Mobile_Poster_v11.png"/><link rel="preload" as="image" href="img/2018-11-01_NIPS-2018-workshop_ViGIL-SceneGraphTransforming_Poster_v16.png"/><link rel="preload" as="image" href="img/2018-10-08_NIPS-2018-workshop_MLOSS_Poster_v18.png"/><link rel="preload" as="image" href="img/2018-10-20_NIPS-2018-workshop_CDNNRIA-Compression_Poster_v11.png"/><link rel="preload" as="image" href="img/2017-12-08_NIPS-2017-ViGIL-Poster_v12.png"/><link rel="stylesheet" href="_next/static/css/c9279efc8fbe65c5.css" data-precedence="next"/><link rel="preload" href="_next/static/chunks/webpack-fbdde187f9fd1a50.js" as="script" fetchPriority="low"/><script src="_next/static/chunks/fd9d1056-c9f5cfab2eb2f33d.js" async=""></script><script src="_next/static/chunks/596-fd0d35f230db4162.js" async=""></script><script src="_next/static/chunks/main-app-3d617632d76638d6.js" async=""></script><title>Red Dragon AI - AI &amp; Machine Learning Training Singapore</title><meta name="description" content="Generated by RDAI App"/><script src="_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body class="__className_aaf875"><div><main class="flex-1 "><div><div class="min-h-screen bg-white"><div class=" min-h-8vh "><div class=" py-6 px-5  flex items-center justify-between max-w-full  mx-auto bg-black"><h1 class="text-lg ml-8 font-semibold text-white cursor-pointer  "><img alt="Red Dragon Logo" loading="lazy" width="200" height="50" decoding="async" data-nimg="1" class=" object-contain" style="color:transparent" srcSet="_next/image?url=%252Fimg%252FRedDragon_logo_260x39.png&amp;w=256&amp;q=75amp;q_next/image?url=%252Fimg%252FRedDragon_logo_260x39.png&amp;w=640&amp;q=75p;w=640&amp;q=75 2x" src="_next/image?url=%252Fimg%252FRedDragon_logo_260x39.png&amp;w=640&amp;q=75"/></h1><div class="flex items-center justify-end space-x-4"><ul class=" xs:hidden md:flex flex-grow-1 flex-shrink-1 p-0 pr-20 gap-12" style="flex-basis:40remm"><li class="relative transition ease-in-out delay-150   hover:-translate-y-1 hover:scale-110 duration-300"><a class="text-[1.25rem]  font-serif font-normal bg-transparent rounded-full text-slate-200 btn" href="products">Products</a></li><li class="relative transition ease-in-out delay-150   hover:-translate-y-1 hover:scale-110 duration-300"><a class="text-[1.25rem] font-serif font-normal bg-transparent rounded-full text-slate-200 btn " href="research">Research</a></li><li class="relative transition ease-in-out delay-150   hover:-translate-y-1 hover:scale-110 duration-300"><a class="text-[1.25rem] font-serif font-normal bg-transparent rounded-full text-slate-200 btn" href="training">Training</a><div class=" absolute bottom-[-20%] left-0 right-0 h-[0.3rem] bg-red-600 z-[1] " style="width:0%"></div></li></ul><div class="sm:block md:hidden  items-center justify-end space-x-4 relative z-50"><button class="text-white self-center mr-12"><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 hover:text-gray-200" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button><div class=" p-5 absolute bg-gray-100 shadow-xl right-0 top-[50px] rounded-xl hidden"><ul class="flex flex-col gap-3"><li class="cursor-pointer px-3 list-none">Products</li><li class="cursor-pointer px-3 list-none">Research</li><li class="cursor-pointer px-3 list-none">Training</li></ul></div></div></div></div></div><main class="text-white w-full flex flex-col justify-center items-center"><div class=" text-[#555] w-[74%] m-auto font-normal text-base font-sans leading-6 "><div class=" flex flex-wrap items-center  flex-row justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2019-08-01_EMNLP-2019-workshop_TextGraphs_v07_with-logo.png" alt="Shared Task: Language Model Assisted Explanation Generation - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">EMNLP-IJCNLP 2019 : WNGT workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Shared Task: Language Model Assisted Explanation Generation</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">The TextGraphs-13 Shared Task on Explanation Regeneration asked participants to develop methods to reconstruct gold explanations for elementary science questions. Red Dragon AI&#x27;s entries used the language of the questions and explanation text directly, rather than a constructing a separate graph-like representation. Our leaderboard submission placed us 3rd in the competition, but we present here three methods of increasing sophistication, each of which scored successively higher on the test set after the competition close.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2019-11-EMNLP_TextGraphs.FINAL3.pdf" target="_blank" rel="noreferrer">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://colab.research.google.com/github/mdda/worldtree_corpus/blob/textgraphs/TextGraphs_Workshop_Code.ipynb" target="_blank" rel="noreferrer">Open in Colab</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://arxiv.org/abs/1911.08976" target="_blank" rel="noreferrer">View on Arxiv</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row-reverse justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2019-08-01_EMNLP-2019-workshop_WNGT_Paraphrasing_with_Large_LMs_cover_final2.png" alt=" Explanation Generation - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">EMNLP-IJCNLP 2019 : WNGT workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Paraphrasing with Large Language Models</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2019-11-EMNLP_WNGT_Final.pdf">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://arxiv.org/abs/1911.09661">View on Arxiv</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2019-08-01_EMNLP-2019-workshop_TextGraphs_v07_with-logo.png" alt="Unsupervised Natural Question Answering with a Small Model - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">EMNLP-IJCNLP 2019 : FEVER workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Unsupervised Natural Question Answering with a Small Model</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">The recent (2019-02) demonstration of the power of huge language models such as GPT-2 to memorise the answers to factoid questions raises questions about the extent to which knowledge is being embedded directly within these large models. This short paper describes an architecture through which much smaller models can also answer such questions - by making use of &#x27;raw&#x27; external knowledge. The contribution of this work is that the methods presented here rely on unsupervised learning techniques, complementing the unsupervised training of the Language Model. The goal of this line of research is to be able to add knowledge explicitly, without extensive training.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2019-11-EMNLP_FEVER_FINAL.pdf">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://arxiv.org/abs/1911.08340">View on Arxiv</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row-reverse justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2018-10-26_NIPS-2018-workshop_MLPCD2-Mobile_Poster_v11.png" alt="Transformer to CNN: Improved Text Classification for Edge Devices - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">NeurIPS 2018 : MLPCD2 workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Transformer to CNN: Improved Text Classification for Edge Devices</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">As Deep Learning and NLP models advance they also become more complicated and computationally heavy. This limits the ability of developers to use these models at the edge on phones and low power devices. In this paper, we introduce a new CNN architecture which can be trained by a distillation process from a large-scale model such as OpenAI&#x27;s Transformer architecture. This student model is then small enough and fast enough to be run on phones. The model can then achieve 300x inference speedup and 39x reduction in parameter count and in some cases, the student model&#x27;s performance surpasses its teacher on the studied tasks.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2019-11-EMNLP_FEVER_FINAL.pdf">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="research">Open in Colab</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2018-11-01_NIPS-2018-workshop_ViGIL-SceneGraphTransforming_Poster_v16.png" alt="Scene Graph Parsing by Attention Graph - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">NeurIPS 2018 : ViGIL workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Scene Graph Parsing by Attention Graph</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">Scene graph representations, which form a graph of visual object nodes together with their attributes and relations, have proved useful across a variety of vision and language applications. Recent work in the area has used Natural Language Processing dependency tree methods to automatically build scene graphs. In this work, we present an `Attention Graph&#x27; mechanism that can be trained end-to-end, and produces a scene graph structure that can be lifted directly from the top layer of a standard Transformer model. The scene graphs generated by our model achieve an F-score similarity of 52.21% to ground-truth graphs on the evaluation set using the SPICE metric, surpassing the best previous approaches by 2.5%.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2018-12-NeurIPS_ViGIL-workshop_CAMERA-READY.pdf">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://github.com/mdda/deep-learning-workshop/tree/master/notebooks/work-in-progress/2018-10_SceneGraphParsing">Read Code</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://arxiv.org/abs/1909.06273">View on Arxiv</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row-reverse justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2018-10-08_NIPS-2018-workshop_MLOSS_Poster_v18.png" alt="Building, growing and sustaining ML communities - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">NeurIPS 2018 : MLOSS workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Building, growing and sustaining ML communities</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">While there are multiple research-based groups for the ML community around the world, the adoption of these skills by a broader base of developers will require new communities that reach beyond researchers to flourish at a large scale. The Singapore TensorFlow &amp; Deep Learning community is a group of over 3,000 people from different backgrounds and levels that is pushing the adoption of ML in South-East Asia, via monthly in-person meetings, guest talks, and special events. In the proposed short talk, we will present some of the challenges, lessons learned and solutions found to building machine learning communities at scale.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2018-12-NeurIPS_ViGIL-workshop_CAMERA-READY.pdf">Read Paper</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2018-10-20_NIPS-2018-workshop_CDNNRIA-Compression_Poster_v11.png" alt="Transformer to CNN: Label-scarce distillation for efficient text classification - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">NeurIPS 2018 : CDNNRIA workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Transformer to CNN: Label-scarce distillation for efficient text classification</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">Significant advances have been made in Natural Language Processing (NLP) modelling since the beginning of 2018. The new approaches allow for accurate results, even when there is little labelled data, because these NLP models can benefit from training on both task-agnostic and task-specific unlabelled data. However, these advantages come with significant size and computational costs. This workshop paper outlines how our proposed convolutional student architecture, having been trained by a distillation process from a large-scale model, can achieve 300x inference speedup and 39x reduction in parameter count. In some cases, the student model performance surpasses its teacher on the studied tasks.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2018-12-NeurIPS_CDNNRIA-workshop_FINAL.pdf">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://colab.research.google.com/drive/1NDctmeKYdJHnPJcNmEoLfJeE77md6yRt">Open in Colab</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://arxiv.org/abs/1909.03508">View on Arxiv</a></button></div></div></div><div class=" flex flex-wrap items-center  flex-row-reverse justify-start "><div class="min-w-[4rem] p-[1rem] pb-[2rem]" style="flex:3 0 16rem"><img class="cursor-pointer m-w-full h-auto align-middle border-none cursor-ooin" src="img/2017-12-08_NIPS-2017-ViGIL-Poster_v12.png" alt="Relationships from Entity Stream - poster"/></div><div class=" max-w-full p-[1rem] pr-[2rem] " style="flex:7 0 30rem"><div><div class=" inline h-[2px] w-[30px] text-red-600 "></div><span class="text-[1.5rem] font-sans leading-6 text-[#555] font-bold ">NIPS 2017 : ViGIL workshop</span></div><div class="text-[#000] font-semibold text-base font-sans leanding-4 "><span>Relationships from Entity Stream</span></div><div class="text-sm leading-4 p-1 pb-2 pr-0 text-justify">Relational reasoning is a central component of intelligent behavior, but has proven difficult for neural networks to learn. The Relation Network (RN) module was recently proposed by DeepMind to solve such problems, and demonstrated state-of-the-art results on a number of datasets. However, the RN module scales quadratically in the size of the input, since it calculates relationship factors between every patch in the visual field, including those that do not correspond to entities.  In this paper, we describe an architecture that enables relationships to be determined from a stream of entities obtained by an attention mechanism over the input field. The model is trained end-to-end, and demonstrates equivalent performance with greater interpretability while requiring only a fraction of the model parameters of the original RN module.</div><div class=" flex justify-start mt-4"><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://reddragon.ai/downloads/2017-12-NIPS_ViGIL-workshop_mdda.pdf">Read Paper</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://github.com/mdda/relationships-from-entity-stream">Code on Github</a></button><button class="transition ease-in-out delay-150 hover:-translate-y-1 hover:scale-110 duration-300 hover:bg-red-500 text-white my-0 mx-[0.5rem] p-[0.75rem] bg-[#de0800] text-sm font-semibold rounded-xl " style="border:1px solid #de0800"><a href="https://arxiv.org/abs/1909.03315">View on Arxiv</a></button></div></div></div></div><footer class="relative bg-[#1C1A17] pt-8 pb-6 w-full"><div class="container mx-auto px-12"><div class="flex flex-wrap text-left lg:text-left"><div class="w-full lg:w-6/12 px-4"><h4 class="text-3xl fonat-semibold text-white hidden">Let&#x27;s keep in touch!</h4><h5 class="text-lg mt-0 mb-2 text-white hidden">Find us on any of these platforms, we respond 1-2 business days.</h5><div class=" mt-6 lg:mb-0 mb-6 flex items-center"><button class="transition ease-in-out delay-150   hover:-translate-y-1 hover:scale-110 duration-300 hidden group flex bg-transparent text-white shadow-lg font-normal h-10 w-10 items-center justify-center align-center rounded-full outline-none focus:outline-none mr-2" type="button"><svg width="32" height="32" class="rounded-full" viewBox="0 0 155 155" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M154.59 0H0.0453034C0.020283 0 0 0.020283 0 0.0453034V154.59C0 154.615 0.020283 154.636 0.0453034 154.636H154.59C154.615 154.636 154.636 154.615 154.636 154.59V0.0453034C154.636 0.020283 154.615 0 154.59 0Z" class="fill-black group-hover:fill-red-600"></path><path d="M107.399 99.6675L110.842 77.3178H89.3987V62.8207C89.3987 56.7199 92.3888 50.7398 101.993 50.7398H111.748V31.7124C111.748 31.7124 102.899 30.2023 94.4425 30.2023C76.7742 30.2023 65.2369 40.8939 65.2369 60.2837V77.3178H45.6054V99.6675H65.2369V154.636H89.3987V99.6675H107.399Z" fill="white"></path></svg></button><a target="_blank" class="transition ease-in-out delay-150   hover:-translate-y-1 hover:scale-110 duration-300 group flex  bg-transparent text-red-600 shadow-lg font-normal h-10 w-10 items-center justify-center align-center rounded-full outline-none focus:outline-none mr-2" type="button" href="https://twitter.com/RedDragonAI"><svg width="32" height="32" class="rounded-full " viewBox="0 0 155 155" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M154.59 0H0.0453034C0.020283 0 0 0.020283 0 0.0453034V154.59C0 154.615 0.020283 154.636 0.0453034 154.636H154.59C154.615 154.636 154.636 154.615 154.636 154.59V0.0453034C154.636 0.020283 154.615 0 154.59 0Z" class="fill-black group-hover:fill-red-600"></path><path d="M131.984 45.9075C128.403 48.2826 124.2 49.5437 119.903 49.5318C124.497 46.8257 127.936 42.5264 129.568 37.4509C125.76 40.6698 120.962 42.4825 115.977 42.5852C112.69 39.219 108.4 37.0095 103.751 36.2882C99.1022 35.567 94.3446 36.3729 90.1924 38.5851C86.0403 40.7973 82.7176 44.2964 80.723 48.5573C78.7284 52.8183 78.1694 57.6111 79.13 62.2167C70.6713 61.7701 62.4012 59.5496 54.8565 55.6992C47.3118 51.8488 40.6612 46.4547 35.3367 39.867C32.7311 44.5817 31.9683 50.0957 33.1958 55.3407C34.4234 60.5857 37.554 65.1884 41.9812 68.2572C38.6399 68.3044 35.3328 67.5809 32.3164 66.143C32.3977 71.0081 34.1082 75.7054 37.1745 79.4834C40.2407 83.2615 44.4855 85.902 49.2297 86.9826C46.0716 87.8073 42.7684 87.9105 39.565 87.2846C40.9469 91.5946 43.6392 95.3659 47.2665 98.0729C50.8939 100.78 55.2755 102.288 59.8005 102.386C50.2437 108.183 39.2657 111.215 28.0881 111.144C37.4494 117.256 48.3231 120.65 59.4988 120.948C70.6744 121.246 81.7135 118.436 91.3873 112.832C101.061 107.229 108.99 99.05 114.291 89.2072C119.592 79.3643 122.058 68.2434 121.413 57.0823C125.399 53.8231 128.951 50.0677 131.984 45.9075Z" fill="white"></path></svg></a><a target="_blank" class="transition ease-in-out delay-150   hover:-translate-y-1 hover:scale-110 duration-300 group  flex bg-white    shadow-lg font-normal h-8 w-8 items-center justify-center align-center rounded-full outline-none focus:outline-none mr-2" type="button" href="https://github.com/reddragonai"><svg width="32" height="32" viewBox="0 0 155 155" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M138.068 0H16.5681C7.42113 0 0 7.42113 0 16.5681V138.068C0 147.215 7.42113 154.636 16.5681 154.636H138.068C147.215 154.636 154.636 147.215 154.636 138.068V16.5681C154.636 7.42113 147.215 0 138.068 0ZM95.7153 132.441C92.8159 132.959 91.7459 131.164 91.7459 129.68C91.7459 127.816 91.8149 118.289 91.8149 110.592C91.8149 105.207 90.02 101.79 87.9145 99.9954C100.686 98.5802 114.147 96.8199 114.147 74.7636C114.147 68.4815 111.904 65.3405 108.245 61.302C108.832 59.8178 110.799 53.7083 107.658 45.7694C102.86 44.2852 91.884 51.9479 91.884 51.9479C87.3277 50.6708 82.3918 50.015 77.5249 50.015C72.6581 50.015 67.7221 50.6708 63.1659 51.9479C63.1659 51.9479 52.1895 44.2852 47.3917 45.7694C44.2507 53.6738 46.1836 59.7833 46.8049 61.302C43.1461 65.3405 41.4203 68.4815 41.4203 74.7636C41.4203 96.7163 54.2951 98.5802 67.0663 99.9954C65.4095 101.48 63.9253 104.034 63.4075 107.693C60.1284 109.177 51.7408 111.731 46.7359 102.895C43.5948 97.4412 37.9341 96.9925 37.9341 96.9925C32.3423 96.9234 37.5544 100.513 37.5544 100.513C41.2822 102.239 43.9055 108.866 43.9055 108.866C47.2536 119.118 63.2695 115.666 63.2695 115.666C63.2695 120.464 63.3385 128.265 63.3385 129.68C63.3385 131.164 62.303 132.959 59.369 132.441C36.5879 124.813 20.6411 103.136 20.6411 77.8011C20.6411 46.1146 44.872 22.0563 76.5585 22.0563C108.245 22.0563 133.926 46.1146 133.926 77.8011C133.96 103.136 118.496 124.848 95.7153 132.441ZM61.8543 111.351C61.1984 111.49 60.5771 111.213 60.5081 110.765C60.4391 110.247 60.8878 109.798 61.5436 109.66C62.1994 109.591 62.8207 109.867 62.8898 110.316C62.9933 110.765 62.5446 111.213 61.8543 111.351ZM58.5752 111.041C58.5752 111.49 58.0574 111.869 57.3671 111.869C56.6077 111.938 56.0899 111.559 56.0899 111.041C56.0899 110.592 56.6077 110.212 57.298 110.212C57.9539 110.143 58.5752 110.523 58.5752 111.041ZM53.8463 110.661C53.7083 111.11 53.0179 111.317 52.4312 111.11C51.7753 110.972 51.3266 110.454 51.4647 110.005C51.6028 109.557 52.2931 109.35 52.8799 109.488C53.5702 109.695 54.0189 110.212 53.8463 110.661ZM49.6008 108.797C49.2901 109.177 48.6343 109.108 48.1165 108.59C47.5988 108.141 47.4607 107.486 47.8059 107.175C48.1165 106.795 48.7724 106.864 49.2901 107.382C49.7388 107.831 49.9114 108.521 49.6008 108.797ZM46.4597 105.656C46.1491 105.863 45.5623 105.656 45.1826 105.138C44.8029 104.621 44.8029 104.034 45.1826 103.792C45.5623 103.482 46.1491 103.723 46.4597 104.241C46.8394 104.759 46.8394 105.38 46.4597 105.656ZM44.2161 102.308C43.9055 102.619 43.3877 102.446 43.008 102.101C42.6284 101.652 42.5593 101.134 42.87 100.893C43.1806 100.582 43.6984 100.755 44.0781 101.1C44.4578 101.549 44.5268 102.066 44.2161 102.308ZM41.9035 99.7538C41.7654 100.064 41.3167 100.133 40.937 99.8919C40.4883 99.6848 40.2812 99.3051 40.4193 98.9944C40.5573 98.7873 40.937 98.6838 41.3857 98.8564C41.8345 99.098 42.0416 99.4777 41.9035 99.7538Z" class="fill-black group-hover:fill-red-600"></path></svg></a></div></div><div class="w-full lg:w-6/12 px-4"><div class="flex flex-wrap items-top mb-6"><div class="w-full lg:w-4/12 px-4 ml-auto"><span class="block uppercase text-white text-sm font-semibold mb-2">Useful Links</span><ul class="list-unstyled"><li><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="about">About Us</a></li><li class="hidden"><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="index.html#">Blog</a></li><li class="hidden"><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" target="_blank" href="https://github.com/reddragonai">Github</a></li><li class="hidden"><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="index.html#">Free Products</a></li></ul></div><div class="w-full lg:w-4/12 px-4"><span class="block uppercase text-white text-sm font-semibold mb-2">Other Resources</span><ul class="list-unstyled"><li class="hidden"><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="index.html#">MIT License</a></li><li class="hidden"><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="index.html#">Terms &amp; Conditions</a></li><li class="hidden"><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="index.html#">Privacy Policy</a></li><li><a class="text-white hover:text-red-600 font-semibold block pb-2 text-sm" href="contactus">Contact Us</a></li></ul></div></div></div></div><hr class="my-6 border-blueGray-300"/><div class="flex flex-wrap items-center md:justify-between justify-center"><div class="w-full md:w-4/12 px-4 mx-auto text-center"><div class="text-sm text-red-600 font-semibold py-1">Copyright © <span id="get-current-year">2022-2024 © All Rights Reserved </span><a href="index.html" class="text-red-600 hover:text-gray-800" target="_blank" rel="noreferrer"> RDAI</a>.</div></div></div></div></footer></main></div></div></main></div><script src="_next/static/chunks/webpack-fbdde187f9fd1a50.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/c9279efc8fbe65c5.css\",{\"as\":\"style\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:I{\"id\":7948,\"chunks\":[\"2272:static/chunks/webpack-fbdde187f9fd1a50.js\",\"2971:static/chunks/fd9d1056-c9f5cfab2eb2f33d.js\",\"596:static/chunks/596-fd0d35f230db4162.js\"],\"name\":\"default\",\"async\":false}\n5:I{\"id\":6628,\"chunks\":[\"2272:static/chunks/webpack-fbdde187f9fd1a50.js\",\"2971:static/chunks/fd9d1056-c9f5cfab2eb2f33d.js\",\"596:static/chunks/596-fd0d35f230db4162.js\"],\"name\":\"\",\"async\":false}\n6:I{\"id\":7767,\"chunks\":[\"2272:static/chunks/webpack-fbdde187f9fd1a50.js\",\"2971:static/chunks/fd9d1056-c9f5cfab2eb2f33d."])</script><script>self.__next_f.push([1,"js\",\"596:static/chunks/596-fd0d35f230db4162.js\"],\"name\":\"default\",\"async\":false}\n7:I{\"id\":7920,\"chunks\":[\"2272:static/chunks/webpack-fbdde187f9fd1a50.js\",\"2971:static/chunks/fd9d1056-c9f5cfab2eb2f33d.js\",\"596:static/chunks/596-fd0d35f230db4162.js\"],\"name\":\"default\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c9279efc8fbe65c5.css\",\"precedence\":\"next\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"O8L84BgJBu6UQ72KLFLZ3\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/research\",\"initialTree\":[\"\",{\"children\":[\"research\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L4\"],\"globalErrorComponent\":\"$5\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_aaf875\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[null,\"$L8\",null],\"segment\":\"research\"},\"styles\":[]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"a:I{\"id\":4839,\"chunks\":[\"2272:static/chunks/webpack-fbdde187f9fd1a50.js\",\"2971:static/chunks/fd9d1056-c9f5cfab2eb2f33d.js\",\"596:static/chunks/596-fd0d35f230db4162.js\"],\"name\":\"default\",\"async\":false}\nb:I{\"id\":5926,\"chunks\":[\"5162:static/chunks/5162-9686277a7dd930db.js\",\"7011:static/chunks/7011-c30dd1689f5b7dc6.js\",\"7051:static/chunks/7051-bc271dae0f53be56.js\",\"9214:static/chunks/app/research/page-cf607100d088b56f.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"div\",null,{\"children\":[\"$\",\"main\",null,{\"className\":\"flex-1 \",\"children\":[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"research\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L9\",[\"$\",\"$La\",null,{\"propsForComponent\":{\"params\":{}},\"Component\":\"$b\"}],null],\"segment\":\"__PAGE__\"},\"styles\":[]}]}]}]}]\n"])</script><script>self.__next_f.push([1,"4:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Red Dragon AI - AI \u0026 Machine Learning Training Singapore\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Generated by RDAI App\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script></body></html>